## Deployment

As much as possible of deployment of the ATLAS system has been scripted, via a combination of Terraform (using the Azure Resource Manager provider), and Azure Devops .yml scripts.
Atlas is supported in an Azure environment, built and deployed using Azure Devops - to change either would require some custom changes to the codebase.

Note that two terraform scripts are used. The first, "terraform/core", covers the majority of ATLAS infrastructure. 
The second, "terraform/webhooks", covers any webhooks that need setting up for e.g. eventGrid triggered functions. As webhooks need to send a 
handshake request to the target as part of set-up, the relevant webhooks must have been deployed before this script can be run.

The following are the steps that are required to be taken manually when deploying ATLAS to a new environment.

### Azure Configuration

- An *Azure subscription* must exist into which the Atlas system will be deployed.
- An *Azure storage account* must be available for Terraform to use as a backend.
- An *App Registration* should be created within Azure Active Directory, used to by Terraform for authentication.

### Azure Devops Configuration

- A variable group named "terraform" should be created, with the following variables:
  - *ARM_ACCESS_KEY*
    - storage account access key for the storage-driven terraform backend
  - *ARM_CLIENT_ID*
  - *ARM_CLIENT_SECRET*
  - *ARM_TENANT_ID*
    - Details available from the azure AD app registration
- New Devops build pipelines should be created, using the checked in `<pipeline>.yml` files.
- An Azure service connection should be set up to the target Azure subscription, scoped to the new resource group for this Atlas installation
  - Due to the restriction by resource group, terraform must be run before this can be set up - either via a partial release or manually
- A Devops release should be manually created
  - The following steps should be defined:
    - Apply terraform
    - Run database migrations
      - Terraform will need to be run for the first time before these can be run, to set up the database server + databases.
      - The server connection details will need to be set manually once terraform has been run.
      - For login details, the server admin details required as release variables can be used.
    - Release azure function apps
    - Apply terraform-webhooks
  - Release variables should be set up for each target environment. Expected variables are defined in `variables.tf`. Those without default values are required.
    - Note that the variable "API_KEY" is slightly unusual: the host keys for azure functions cannot be set by terraform, so this cannot be set before the first release.
      The only usage is via a terraform export, in case consumers of the matching function choose to access the host key via terraform remote states.
    - A default host key will be automatically generated by azure, and is used via a `X-Functions-Key` header to authenticate HTTP triggered functions. New keys can also be generated.
      The key specified in the app settings will be exported as a Terraform output variable - consumers of this output via remote states will need to know how to use this api

### Terraform Configuration

- Before terraform can be run for the first time, a new terraform workspace should be manually created. During the release step, this new workspace should be selected.
  - In development, we needed to create this workspace as a one-off command in our Devops release, before removing it for future releases.
  - TODO: ATLAS-216: Ensure this can be run as a one-off locally, without needing to change and revert the Devops release pipeline
- You will also need to register any relevant resource providers. See notes in `.\terraform\main.tf`.
- All Atlas infrastructure is controlled via terraform scripts. If any specific naming or configuration changes are required for your installation, such changes should be made to the terraform scripts in a fork of the repository - changing them manually in Azure will lead to the changes being reverted on the next deployment to that environment.

### Manual Azure Configuration (Post-terraform)

Once terraform has created ATLAS resources for the first time, certain actions must be performed manually on these resources, as they are either not available or not recommended as part of the terraform scripting.

- Azure Function Host Keys
  - Any host keys other than the default should be created manually
  - Master host keys for any functions with Webhooks must be specified as variables in the release pipeline.
    - The webhooks terraform script will fail without these values. Running the first terraform script will create the functions and allow you to fetch the host keys.
    - TODO: ATLAS-379: Fetch this via Azure CLI as part of release
- Azure SQL Permissions
  - Service Accounts
    - Each service (e.g. matching) within ATLAS should have a service account created on the appropriate databases. The username and password for such accounts should then be set as a variable in the release pipeline.
    - Passwords should be created by a Password Generator, such as <https://passwordsgenerator.net/>.Sensible generation settings might be:
      - 16 characters
      - Upper, Lower, Numbers, (maybe not Symbols)
      - Exclude ambiguous letters.
      - Exclude ambiguous Symbols (if using).
    - By default, `db_datareader` and `db_datawriter` will be necessary for a given component to access its corresponding database(s)  
    - Note that the user for the matching component to access the *transient matching databases* (a and b) will need to be granted `db_owner` permission, as a `truncate table` command is used in the full data refresh, which requires elevated permissions

      ```sql
      CREATE USER [USERNAME] WITH PASSWORD = 'PASSWORD'
      ALTER ROLE db_datareader ADD MEMBER [USERNAME]
      -- uncomment more lines as indicated in table below
      -- ALTER ROLE db_datawriter ADD MEMBER [USERNAME]
      -- ALTER ROLE db_owner ADD MEMBER [USERNAME]
      ```

    Expected Access Requirements:

      |Database             |User            |Permissions                 |
      |---------------------|----------------|----------------------------|
      |Matching - Transient |Matching        |db-owner/db-writer/db-reader|
      |Matching - Persistent|Matching        |db-owner/db-writer/db-reader|
      |Match Prediction     |Match Prediction|db-writer/db-reader         |
      |Donor Import         |Donor Import    |db-writer/db-reader         |
      |Donor Import         |Matching        |db-reader                   |

  - Active Directory (Optional)
        - If you would like to be able to access the database server using Active Directory authentication, this should be manually configured
  - IP Whitelisting (Optional)
    - By default, only other azure services will be allowed to access the database server through the firewall. For development access, any known IP addresses should be manually added to the IP whitelist in Azure.  

### System Tests

The system tests require some Azure resources of their own - primarily SQL databases. For now these are manually created, in Azure.
// TODO: ATLAS-314: Create these resources using terraform.

## Releasing to multiple environments

The expected use case for multiple environments is a development -> uat -> live route.
This section details how to set up for such a case.

- (Optional) You may prefer each environment to exist on a different Azure subscription. If so, create this new subscription and use this subscription ID when running terraform
- A new terraform workspace should be created for each environment
- New release stages can be created within one Azure Devops release pipeline. This can be useful for ensuring the same build artifacts that were
tested in a test environment are deployed to live
- New service connections will need to be set up in Azure Devops for each resource group